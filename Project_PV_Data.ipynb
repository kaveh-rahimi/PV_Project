{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Source:\n",
    "https://github.com/mohammad-q-cells/test-data\n",
    "\n",
    "Items to addrress:\n",
    "\n",
    "1.\tData preprocessing and cleaning: What are the preprocessing steps you have taken to clean the data?\n",
    "How do you handle missing values?. Please describe and list all the methods and rationale behind them.\n",
    "Include image where it is necessary\n",
    "\n",
    "2.\tExplore the data and generate insights from data. It is open ended. Please describe and list all the methods.\n",
    "Include image where it is necessary\n",
    "\n",
    "3.\tWhat statistical test did you perform on the data to check its stationarity, co-integration etc.\n",
    "Please state your reasoning behind a particular test. (please include results and pictures)\n",
    "\n",
    "4.\tDevelop a prediction model to forecast energy production for the next 30 days.\n",
    "a. what type of model you used and describe the reasons behind using this particular model \n",
    "b. what are features, how do you come up with these features \n",
    "c. how do you validate the results?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I personally do a quick data exploration to get a feeling about the data before dealing with data cleansing and missing data points.\n",
    "In this way, I gain valuable hints for data cleansing and I can think of ideas for feature engineering.\n",
    "Using Pandas let me do this preliminary data exploration easily and quickly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv file via Pandas and checking the format\n",
    "url='https://raw.githubusercontent.com/mohammad-q-cells/test-data/main/data-for-test2.csv'\n",
    "df= pd.read_csv(url)  \n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a feeling about the data types and their length\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "outliers were detected!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I also visualize the data to get a feeling about the data (features and the target variable(s))\n",
    "f, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axs[0,0].hist(df['irradiance'])\n",
    "axs[0,0].set(xlabel='irradiance (W/m^2)', ylabel='count')\n",
    "axs[0,1].hist(df['humidity'])\n",
    "axs[0,1].set(xlabel='humidity', ylabel='count')\n",
    "axs[1,0].hist(df['temperature'])\n",
    "axs[1,0].set(xlabel='temperature (C)', ylabel='count')\n",
    "axs[1,1].hist(df['energy'])\n",
    "axs[1,1].set(xlabel='energy (kW)', ylabel='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obviously some large values (outliers) are preventing the histogram to effectively represent the population distribution.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I always check minimum and maximum values and use my domain knowledge to see if the data makes sense.\n",
    "If I do not have the domain knowledge, I try to gain some insight about the quantities represented in the data and their limits)\n",
    "Obviously, as seen in the histograms, the maximum values for irradiance, temperature, and energy has a huge difference with the mean values.\n",
    "So those data points need to be taken care of. Other centers of data such as median and mode also can be used.\n",
    "In addition, if we assume the data is normal, we identify the data points with more than 3 standard deviations from the mean as outliers.\n",
    "\n",
    "To handle missing data or wrong data values different methods can be used such as:\n",
    "1) filling them with a specific value which is not in the range of the data (for example -99999)\n",
    "2) filling them with mean or median values\n",
    "3) filling them with the previous values or next values\n",
    "4) filling them with interpolation (I used linear interpolation to handle some wrong measurements in this project)\n",
    "5) Keeping them, but adding a column with a binary value (true or false) showing that something was not right about this data point.\n",
    "6) Removing the row/column with at least one missing/wrong value\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see if there is any missing values\n",
    "df[df.isnull().T.any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "obviously here we do not have to deal with NaN or null values\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1=df['irradiance'].idxmax(axis=0, skipna=True)\n",
    "index2=df['temperature'].idxmax(axis=0, skipna=True)\n",
    "index3=df['energy'].idxmax(axis=0, skipna=True)\n",
    "\n",
    "print(df[index1:index1+1])\n",
    "print(df[index2:index2+1])\n",
    "print(df[index3:index3+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "based on previous comments about maximum values, data points on vicinity of 12/2/16 10:15 and 4/28/16 11:50 need attention.\n",
    "\"\"\"\n",
    "df[index1-5:index1+5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[index3-5:index3+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this stage, 5 values need to be taken care of. 2 temperature and 2 irradiance values on 12/2/16 10:15 and\n",
    "1 energy value on 4/28/16 11:50.\n",
    "As discussed above, previous values, next values, or linear regression can be used to estimate the right values.\n",
    "Here I used the average of the previous and next values.\n",
    "This is an iterative process, I will visualize the data and check min and max values to take care of other possible data issues.\n",
    "\"\"\"\n",
    "\n",
    "df['temperature'][index1]=(df['temperature'][index1-1]+df['temperature'][index1+2])/2\n",
    "df['temperature'][index1+1]=(df['temperature'][index1-1]+df['temperature'][index1+2])/2\n",
    "\n",
    "df['irradiance'][index1]=(df['irradiance'][index1-1]+df['irradiance'][index1+2])/2\n",
    "df['irradiance'][index1+1]=(df['irradiance'][index1-1]+df['irradiance'][index1+2])/2\n",
    "\n",
    "df['energy'][index3]=(df['energy'][index3-1]+df['energy'][index3+1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axs[0,0].hist(df['irradiance'])\n",
    "axs[0,0].set(xlabel='irradiance (W/m^2)', ylabel='count')\n",
    "axs[0,1].hist(df['humidity'])\n",
    "axs[0,1].set(xlabel='humidity', ylabel='count')\n",
    "axs[1,0].hist(df['temperature'])\n",
    "axs[1,0].set(xlabel='temperature (C)', ylabel='count')\n",
    "axs[1,1].hist(df['energy'])\n",
    "axs[1,1].set(xlabel='energy (kW)', ylabel='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Another large energy value is observed and needs to be taken care of.\n",
    "\"\"\"\n",
    "index4=df['energy'].idxmax(axis=0, skipna=True)\n",
    "\n",
    "print(df[index4:index4+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[index4-5:index4+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['energy'][index4]=(df['energy'][index4-1]+df['energy'][index4+1])/2\n",
    "df[index4-5:index4+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axs[0,0].hist(df['irradiance'])\n",
    "axs[0,0].set(xlabel='irradiance (W/m^2)', ylabel='count')\n",
    "axs[0,1].hist(df['humidity'])\n",
    "axs[0,1].set(xlabel='humidity', ylabel='count')\n",
    "axs[1,0].hist(df['temperature'])\n",
    "axs[1,0].set(xlabel='temperature (C)', ylabel='count')\n",
    "axs[1,1].hist(df['energy'])\n",
    "axs[1,1].set(xlabel='energy (kW)', ylabel='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It seems the manual process of dealing with wrong data points is not effective. so it is a good idea to evaluate the \n",
    "situation and select an automatic approach if number of wrong data points are considerable.\n",
    "\"\"\"\n",
    "\n",
    "df[df['energy'] > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I picked 30 by checking the number of values greater than it, which is a small number.\n",
    "It is a design choice and other values could be used. Some people used 1.5*IQR to detect the outliers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=df[df['energy'] > 30].index\n",
    "for index_temp in temp:\n",
    "    df['energy'][index_temp]=(df['energy'][index_temp-1]+df['energy'][index_temp+1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axs[0,0].hist(df['irradiance'])\n",
    "axs[0,0].set(xlabel='irradiance (W/m^2)', ylabel='count')\n",
    "axs[0,1].hist(df['humidity'])\n",
    "axs[0,1].set(xlabel='humidity', ylabel='count')\n",
    "axs[1,0].hist(df['temperature'])\n",
    "axs[1,0].set(xlabel='temperature (C)', ylabel='count')\n",
    "axs[1,1].hist(df['energy'])\n",
    "axs[1,1].set(xlabel='energy (kW)', ylabel='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It seems at this point the issue of outliers is fairly resolved. Let's visualize the data and see it again.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[149500:156500].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we can see that the feature vector is missing in a time period. all of the methods for dealing with missing data points can be used here.\n",
    "But based on the size of the missing data, I start with removing (dropping) that period.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[150000:155999].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It seems now we can start working with this data. Previously we checked for Null values and there were none.\n",
    "At this stage data cleansing can be considered done (of course we may come back and make changes if needed)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Again by using domain knowledge or checking the correlation coefficients we can figure out the relationship\n",
    "between features and the target variable (PV generation). This part is going to address the follwing questions:\n",
    "\n",
    "\"2.\tExplore the data and generate insights from data. It is open ended. Please describe and list all the methods.\n",
    "Include image where it is necessary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['irradiance'].corr(df['energy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['humidity'].corr(df['energy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['temperature'].corr(df['energy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the results make sense. Increasing the irradiance, increases the PV Generation.\n",
    "Increasing the humidity, decreases the PV Generation.\n",
    "Increasing the temperature (due to sun position), increases the PV Generation. The impact of temperature on solar panels efficiency is not\n",
    "significant compared to impact of irradiance on PV generation.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Kendall and Pearson correlation can also be used based on the distribution of the features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(method='spearman'), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In many cases, feature scaling speeds up learning algorithms for example gradient descent.\n",
    "Standardizing and normalizing can be used for feature scaling. \n",
    "However, for this example, I am going to keep the original data scale.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What statistical test did you perform on the data to check its stationarity, co-integration etc.\n",
    "Please state your reasoning behind a particular test. (please include results and pictures)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Obviously in time-series analysis working with stationary data is much easier. Due to the seasonality of\n",
    "irradiance, temperature, and humidity, it seems the data in different parts of the year is not stationary. Generally\n",
    "climate data samples are considered cyclo-stationary. For example if we only consider the data in one month, then it can be considered stationary.\n",
    "obviously if there is a trend, again data is not stationary. Here I do not consider climate change as a trend.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.\tDevelop a prediction model to forecast energy production for the next 30 days.\n",
    "a. what type of model you used and describe the reasons behind using this particular model \n",
    "b. what are features, how do you come up with these features \n",
    "c. how do you validate the results?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross validation while dealing with time series data to validate the results\n",
    "\"\"\"\n",
    "from IPython.display import Image\n",
    "Image(\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
